{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b6924f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5c9d74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.3.2-bin-hadoop3'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a6c96bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "208fa90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a51e6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d99f2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('learn-pyspark').master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2760ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1 , 'suraj') , (2,'test')]\n",
    "schema = ['id' , 'name']\n",
    "df = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7da86b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|suraj|\n",
      "|  2| test|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "80381da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{'id' : 1,'name' : 'suraj'},{'id' : 2,'name' : 'suraj singh'},]\n",
    "schema1 = ['id' , 'name']\n",
    "df1 = spark.createDataFrame(data=data1,schema=schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2c08fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ede2a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1|      suraj|\n",
      "|  2|suraj singh|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3fbc8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[1,'suraj'],[ 2,'suraj singh']]\n",
    "schema2 = ['id' , 'name']\n",
    "df2 = spark.createDataFrame(data=data2,schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cb0a4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "215e0efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1|      suraj|\n",
      "|  2|suraj singh|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9bd50981",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = [{'id' : 1,'name' : 'suraj'},{'id' : 2,'name' : 'suraj singh'},]\n",
    "schema3 = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType()),\n",
    "    StructField(name=\"name\", dataType=StringType()),\n",
    "])\n",
    "df3 = spark.createDataFrame(data=data3,schema=schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fac6c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c112a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1|      suraj|\n",
      "|  2|suraj singh|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a3e8ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark: 'SparkSession')\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark: 'SparkSession')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      " |      >>> df.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      " |      >>> df2 = spark.read.csv(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |  \n",
      " |  format(self, source: str) -> 'DataFrameReader'\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      " |      >>> df1.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      " |      >>> df2 = spark.read.json(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
      " |      ...     opt1=True, opt2=1, opt3='str')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      " |      ...     'python/test_support/sql/people1.json'])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      " |  \n",
      " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      " |  \n",
      " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |  \n",
      " |  table(self, tableName: str) -> 'DataFrame'\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.createOrReplaceTempView('tmpTable')\n",
      " |      >>> spark.read.table('tmpTable').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello'), Row(value='this')]\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello\\nthis')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ccef786",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs = spark.read.csv('data/tcs-income.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3539a41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TCS_income-statement_Annual_As_Originally_Reported', 'string'),\n",
       " ('2013', 'double'),\n",
       " ('2014', 'double'),\n",
       " ('2015', 'double'),\n",
       " ('2016', 'double'),\n",
       " ('2017', 'double'),\n",
       " ('2018', 'double'),\n",
       " ('2019', 'double'),\n",
       " ('2020', 'double'),\n",
       " ('2021', 'double'),\n",
       " ('2022', 'double'),\n",
       " ('TTM', 'double')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dba69c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "|TCS_income-statement_Annual_As_Originally_Reported|        2013|        2014|        2015|        2016|      2017|      2018|      2019|      2020|      2021|       2022|        TTM|\n",
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "|                                      Gross Profit| 3.629702E11| 4.886067E11| 5.211143E11| 6.226331E11| 5.3537E11| 5.4008E11| 6.5947E11| 6.9092E11| 7.7302E11|  9.1487E11| 1.01628E12|\n",
      "|                                     Total Revenue| 6.298948E11| 8.180936E11| 9.464841E11|1.0864621E12|1.17966E12|1.23104E12|1.46463E12|1.56949E12|1.64177E12| 1.91754E12| 2.16887E12|\n",
      "|                                      Business ...|  6.29903E11| 8.180936E11| 9.464841E11|1.0864621E12|1.17966E12|1.23104E12|1.46463E12|1.56949E12|1.64177E12| 1.91754E12| 2.16887E12|\n",
      "|                                      Excise Taxes|  -8200000.0|        null|        null|        null|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                   Cost of Revenue|-2.669246E11|-3.294869E11|-4.253698E11| -4.63829E11|-6.4429E11|-6.9096E11|-8.0516E11|-8.7857E11|-8.6875E11|-1.00267E12|-1.15259E12|\n",
      "|                                      Cost of G...|  -2.6525E10| -3.08868E10| -3.83583E10| -4.61382E10| -2.808E10|   -2.7E10|  -2.27E10| -1.905E10| -1.462E10|  -1.163E10|  -1.521E10|\n",
      "|                                      Staff Cos...|-2.403996E11|-2.986001E11|-3.870115E11|-4.176908E11|-6.1621E11|-6.6396E11|-7.8246E11|-8.5952E11|-8.5413E11| -9.9104E11|-1.13738E12|\n",
      "|                              Operating Income/...|-1.928942E11|-2.506061E11| -2.94143E11|-3.360862E11|-2.3088E11|  -2.33E11|-2.8497E11|-3.0512E11| -3.462E11| -4.2899E11| -4.9234E11|\n",
      "|                                  Selling, Gene...|-1.199059E11|-1.559034E11|-1.768871E11|-1.939459E11| -9.921E10|-1.0054E11|-1.2651E11|-1.4529E11|-2.1511E11| -2.7909E11|       null|\n",
      "|                                       Staff Costs|  -7.8819E10|-1.062629E11| -1.22232E11|-1.367865E11|      null|      null|      null|      null| -6.401E10|   -8.45E10|       null|\n",
      "|                                          Pensi...|  -7.8819E10|-1.062629E11| -1.22232E11|-1.367865E11|      null|      null|      null|      null| -6.401E10|   -8.45E10|       null|\n",
      "|                                      Legal, Ac...|   -4.6053E9|   -6.1361E9|    -5.963E9|   -6.3983E9| -8.854E10| -8.992E10| -1.133E11|-1.2937E11|-1.3214E11| -1.7409E11|       null|\n",
      "|                                          Other...|        null|        null|        null|        null| -8.854E10| -8.992E10| -1.133E11|-1.2937E11|      null|       null|       null|\n",
      "|                                      Telecommu...|   -7.6691E9|   -8.7404E9| -1.05606E10| -1.10731E10| -1.067E10| -1.062E10| -1.321E10| -1.592E10| -1.896E10|   -2.05E10|       null|\n",
      "|                                      General a...|   -7.9911E9|   -9.2389E9| -1.03257E10| -1.03836E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Insurance...|    -4.455E8|    -6.113E8|    -7.041E8|    -7.525E8|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Rent Expense| -1.16517E10| -1.42127E10| -1.56946E10| -1.69385E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Selling a...|   -8.7242E9| -1.07011E10| -1.14071E10| -1.16134E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                  Depreciation,...| -1.07992E10| -1.34915E10| -1.79869E10| -1.94796E10| -1.987E10| -2.014E10| -2.056E10| -3.529E10| -4.065E10|  -4.604E10|  -4.953E10|\n",
      "|                                      Depreciat...| -1.07992E10| -1.34915E10| -1.79869E10| -1.94796E10| -1.987E10| -2.014E10| -2.056E10| -3.529E10| -4.065E10|  -4.604E10|  -4.953E10|\n",
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "63fd84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_l = spark.read.format('csv').option(key=\"header\" , value=True)\\\n",
    "                                                                    .option(key=\"inferSchema\" , value=True)\\\n",
    "                                                                    .load('data/tcs-income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4b6df9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "|TCS_income-statement_Annual_As_Originally_Reported|        2013|        2014|        2015|        2016|      2017|      2018|      2019|      2020|      2021|       2022|        TTM|\n",
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "|                                      Gross Profit| 3.629702E11| 4.886067E11| 5.211143E11| 6.226331E11| 5.3537E11| 5.4008E11| 6.5947E11| 6.9092E11| 7.7302E11|  9.1487E11| 1.01628E12|\n",
      "|                                     Total Revenue| 6.298948E11| 8.180936E11| 9.464841E11|1.0864621E12|1.17966E12|1.23104E12|1.46463E12|1.56949E12|1.64177E12| 1.91754E12| 2.16887E12|\n",
      "|                                      Business ...|  6.29903E11| 8.180936E11| 9.464841E11|1.0864621E12|1.17966E12|1.23104E12|1.46463E12|1.56949E12|1.64177E12| 1.91754E12| 2.16887E12|\n",
      "|                                      Excise Taxes|  -8200000.0|        null|        null|        null|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                   Cost of Revenue|-2.669246E11|-3.294869E11|-4.253698E11| -4.63829E11|-6.4429E11|-6.9096E11|-8.0516E11|-8.7857E11|-8.6875E11|-1.00267E12|-1.15259E12|\n",
      "|                                      Cost of G...|  -2.6525E10| -3.08868E10| -3.83583E10| -4.61382E10| -2.808E10|   -2.7E10|  -2.27E10| -1.905E10| -1.462E10|  -1.163E10|  -1.521E10|\n",
      "|                                      Staff Cos...|-2.403996E11|-2.986001E11|-3.870115E11|-4.176908E11|-6.1621E11|-6.6396E11|-7.8246E11|-8.5952E11|-8.5413E11| -9.9104E11|-1.13738E12|\n",
      "|                              Operating Income/...|-1.928942E11|-2.506061E11| -2.94143E11|-3.360862E11|-2.3088E11|  -2.33E11|-2.8497E11|-3.0512E11| -3.462E11| -4.2899E11| -4.9234E11|\n",
      "|                                  Selling, Gene...|-1.199059E11|-1.559034E11|-1.768871E11|-1.939459E11| -9.921E10|-1.0054E11|-1.2651E11|-1.4529E11|-2.1511E11| -2.7909E11|       null|\n",
      "|                                       Staff Costs|  -7.8819E10|-1.062629E11| -1.22232E11|-1.367865E11|      null|      null|      null|      null| -6.401E10|   -8.45E10|       null|\n",
      "|                                          Pensi...|  -7.8819E10|-1.062629E11| -1.22232E11|-1.367865E11|      null|      null|      null|      null| -6.401E10|   -8.45E10|       null|\n",
      "|                                      Legal, Ac...|   -4.6053E9|   -6.1361E9|    -5.963E9|   -6.3983E9| -8.854E10| -8.992E10| -1.133E11|-1.2937E11|-1.3214E11| -1.7409E11|       null|\n",
      "|                                          Other...|        null|        null|        null|        null| -8.854E10| -8.992E10| -1.133E11|-1.2937E11|      null|       null|       null|\n",
      "|                                      Telecommu...|   -7.6691E9|   -8.7404E9| -1.05606E10| -1.10731E10| -1.067E10| -1.062E10| -1.321E10| -1.592E10| -1.896E10|   -2.05E10|       null|\n",
      "|                                      General a...|   -7.9911E9|   -9.2389E9| -1.03257E10| -1.03836E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Insurance...|    -4.455E8|    -6.113E8|    -7.041E8|    -7.525E8|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Rent Expense| -1.16517E10| -1.42127E10| -1.56946E10| -1.69385E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                      Selling a...|   -8.7242E9| -1.07011E10| -1.14071E10| -1.16134E10|      null|      null|      null|      null|      null|       null|       null|\n",
      "|                                  Depreciation,...| -1.07992E10| -1.34915E10| -1.79869E10| -1.94796E10| -1.987E10| -2.014E10| -2.056E10| -3.529E10| -4.065E10|  -4.604E10|  -4.953E10|\n",
      "|                                      Depreciat...| -1.07992E10| -1.34915E10| -1.79869E10| -1.94796E10| -1.987E10| -2.014E10| -2.056E10| -3.529E10| -4.065E10|  -4.604E10|  -4.953E10|\n",
      "+--------------------------------------------------+------------+------------+------------+------------+----------+----------+----------+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_l.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "92a5d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_data = [(1,'suraj singh' , 2303) , (2, 'saheel singh', 2303) , (3,'rohan singh', 2303),(1,'suraj singh', 2303) , (2, 'saheel singh', 2303) , (3,'rohan singh', 2303)]\n",
    "students_schema = StructType([StructField(name=\"id\" , dataType=IntegerType()) , StructField(name=\"name\",dataType=StringType()),StructField(name=\"salary\",dataType=LongType())])\n",
    "students_df = spark.createDataFrame(data=students_data,schema=students_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "02a4dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+\n",
      "| id|        name|salary|\n",
      "+---+------------+------+\n",
      "|  1| suraj singh|  2303|\n",
      "|  2|saheel singh|  2303|\n",
      "|  3| rohan singh|  2303|\n",
      "|  1| suraj singh|  2303|\n",
      "|  2|saheel singh|  2303|\n",
      "|  3| rohan singh|  2303|\n",
      "+---+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "132cffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|sur...|  2303|\n",
      "|  2|sah...|  2303|\n",
      "|  3|roh...|  2303|\n",
      "|  1|sur...|  2303|\n",
      "|  2|sah...|  2303|\n",
      "|  3|roh...|  2303|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show(truncate=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4a85287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------\n",
      " id     | 1            \n",
      " name   | suraj singh  \n",
      " salary | 2303         \n",
      "-RECORD 1--------------\n",
      " id     | 2            \n",
      " name   | saheel singh \n",
      " salary | 2303         \n",
      "-RECORD 2--------------\n",
      " id     | 3            \n",
      " name   | rohan singh  \n",
      " salary | 2303         \n",
      "-RECORD 3--------------\n",
      " id     | 1            \n",
      " name   | suraj singh  \n",
      " salary | 2303         \n",
      "-RECORD 4--------------\n",
      " id     | 2            \n",
      " name   | saheel singh \n",
      " salary | 2303         \n",
      "-RECORD 5--------------\n",
      " id     | 3            \n",
      " name   | rohan singh  \n",
      " salary | 2303         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "43a5700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+\n",
      "| id|        name|salary|\n",
      "+---+------------+------+\n",
      "|  1| suraj singh|  2303|\n",
      "|  2|saheel singh|  2303|\n",
      "+---+------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "000e52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------------+\n",
      "| id|        name|salary|       email|\n",
      "+---+------------+------+------------+\n",
      "|  1| suraj singh|  2303| suraj.singh|\n",
      "|  2|saheel singh|  2303|saheel.singh|\n",
      "|  3| rohan singh|  2303| rohan.singh|\n",
      "|  1| suraj singh|  2303| suraj.singh|\n",
      "|  2|saheel singh|  2303|saheel.singh|\n",
      "|  3| rohan singh|  2303| rohan.singh|\n",
      "+---+------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def email(col) :\n",
    "    splt = f.split(students_df.name,' ')\n",
    "    join = f.array_join(splt,'.')\n",
    "    return f.lit(join)\n",
    "students_df.withColumn('email',email(students_df.name)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
